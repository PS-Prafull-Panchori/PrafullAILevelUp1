<h1>Build a local RAG with Ollama</h1>


<h2>Prerequisites</h2>
<ul>
  <li>Python 3.11+</li>
  <li>Download llama3.2 from ollama (LLM model) (ollama pull llama3.2)</li>
  <li>Download mxbai-embed-large from ollama(embedding model)(ollama pull mxbai-embed-large)</li>
</ul>

<h2>Installation</h2>
<h3>1. Clone the repository:</h3>

```
git clone https://github.com/PS-Prafull-Panchori/PrafullAILevelUp1
cd "Project name"
```

<h3>2. Create a virtual environment</h3>

```
python -m venv venv
```

<h3>3. Activate the virtual environment</h3>

```
venv\Scripts\Activate

```

<h3>4. Install libraries</h3>

```
pip install -r requirements.txt
```

<h3>
<ul>
<li><i>If you want to use ChatGPT or Anthropic models, add an API key (not required for Ollama)</i></li>
</ul>

<h2>Executing the scripts</h2>

- Open a terminal in VS Code

- Execute the following command:

```
python .\main.py

```


